"""Ingestion Traces page â€“ browse ingestion trace history with per-stage detail.

Layout:
1. Trace list (reverse-chronological, filtered to trace_type=="ingestion")
2. Pipeline overview: source file, total time, stage timing waterfall
3. Per-stage detail tabs:
   ğŸ“„ Load    â€“ raw document text preview
   âœ‚ï¸ Split   â€“ chunk list with text
   ğŸ”„ Transform â€“ before/after diff, enrichment metadata
   ğŸ”¢ Embed   â€“ vector stats
   ğŸ’¾ Upsert  â€“ stored IDs
"""

from __future__ import annotations

import logging
from typing import Any, Dict, List, Optional

import streamlit as st

from src.observability.dashboard.services.trace_service import TraceService

logger = logging.getLogger(__name__)


def render() -> None:
    """Render the Ingestion Traces page."""
    st.header("ğŸ”¬ æ‘„å–è¿½è¸ª")

    svc = TraceService()
    traces = svc.list_traces(trace_type="ingestion")

    if not traces:
        st.info("æš‚æ— æ‘„å–è¿½è¸ªè®°å½•ã€‚è¯·å…ˆæ‰§è¡Œä¸€æ¬¡æ‘„å–æ“ä½œï¼")
        return

    st.subheader(f"ğŸ“‹ è¿½è¸ªå†å² ({len(traces)})")

    for idx, trace in enumerate(traces):
        trace_id = trace.get("trace_id", "unknown")
        started = trace.get("started_at", "â€”")
        total_ms = trace.get("elapsed_ms")
        total_label = f"{total_ms:.0f} ms" if total_ms is not None else "â€”"
        meta = trace.get("metadata", {})
        source_path = meta.get("source_path", "â€”")

        # Build expander title
        file_name = source_path.rsplit("/", 1)[-1].rsplit("\\", 1)[-1] if source_path != "â€”" else "â€”"
        expander_title = f"ğŸ“„ **{file_name}** Â· {total_label} Â· {started[:19]}"

        with st.expander(expander_title, expanded=(idx == 0)):
            timings = svc.get_stage_timings(trace)
            stages_by_name = {t["stage_name"]: t for t in timings}

            # â”€â”€ 1. Overview metrics â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            st.markdown("#### ğŸ“Š æµæ°´çº¿æ¦‚è§ˆ")
            st.caption(f"æ¥æº: `{source_path}`")

            load_d = stages_by_name.get("load", {}).get("data", {})
            split_d = stages_by_name.get("split", {}).get("data", {})
            transform_d = stages_by_name.get("transform", {}).get("data", {})
            embed_d = stages_by_name.get("embed", {}).get("data", {})
            upsert_d = stages_by_name.get("upsert", {}).get("data", {})

            c1, c2, c3, c4, c5 = st.columns(5)
            with c1:
                st.metric("æ–‡æ¡£é•¿åº¦", f"{load_d.get('text_length', 0):,} å­—ç¬¦")
            with c2:
                st.metric("åˆ†å—æ•°", split_d.get("chunk_count", 0))
            with c3:
                st.metric("å›¾ç‰‡æ•°", load_d.get("image_count", 0))
            with c4:
                st.metric("å‘é‡æ•°", upsert_d.get("vector_count", 0))
            with c5:
                st.metric("æ€»è€—æ—¶", total_label)

            st.divider()

            # â”€â”€ 2. Stage timing waterfall â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            # Filter to main pipeline stages only (not sub-stages)
            main_stages = [
                t for t in timings
                if t["stage_name"] in ("load", "split", "transform", "embed", "upsert")
            ]
            if main_stages:
                st.markdown("#### â±ï¸ å„é˜¶æ®µè€—æ—¶")
                chart_data = {t["stage_name"]: t["elapsed_ms"] for t in main_stages}
                st.bar_chart(chart_data, horizontal=True)
                st.table([
                    {
                        "é˜¶æ®µ": t["stage_name"],
                        "è€—æ—¶ (ms)": round(t["elapsed_ms"], 2),
                    }
                    for t in main_stages
                ])

            # â”€â”€ Diagnostics â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            _render_ingestion_diagnostics(stages_by_name, load_d, split_d, transform_d, embed_d, upsert_d)

            st.divider()

            # â”€â”€ 3. Per-stage detail tabs â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            st.markdown("#### ğŸ” é˜¶æ®µè¯¦æƒ…")

            tab_defs = []
            if "load" in stages_by_name:
                tab_defs.append(("ğŸ“„ åŠ è½½", "load"))
            if "split" in stages_by_name:
                tab_defs.append(("âœ‚ï¸ åˆ†å—", "split"))
            if "transform" in stages_by_name:
                tab_defs.append(("ğŸ”„ è½¬æ¢", "transform"))
            if "embed" in stages_by_name:
                tab_defs.append(("ğŸ”¢ ç¼–ç ", "embed"))
            if "upsert" in stages_by_name:
                tab_defs.append(("ğŸ’¾ å­˜å‚¨", "upsert"))

            if tab_defs:
                tabs = st.tabs([label for label, _ in tab_defs])
                for tab, (label, key) in zip(tabs, tab_defs):
                    with tab:
                        stage = stages_by_name[key]
                        data = stage.get("data", {})
                        elapsed = stage.get("elapsed_ms")
                        if elapsed is not None:
                            st.caption(f"â±ï¸ {elapsed:.1f} ms")

                        if key == "load":
                            _render_load_stage(data, trace_idx=idx)
                        elif key == "split":
                            _render_split_stage(data, trace_idx=idx)
                        elif key == "transform":
                            _render_transform_stage(data, trace_idx=idx)
                        elif key == "embed":
                            _render_embed_stage(data)
                        elif key == "upsert":
                            _render_upsert_stage(data)
            else:
                st.info("æ— é˜¶æ®µè¯¦æƒ…ã€‚")


def _render_ingestion_diagnostics(
    stages_by_name: Dict[str, Any],
    load_d: Dict[str, Any],
    split_d: Dict[str, Any],
    transform_d: Dict[str, Any],
    embed_d: Dict[str, Any],
    upsert_d: Dict[str, Any],
) -> None:
    """Render diagnostic hints for ingestion pipeline stages."""
    expected = ["load", "split", "transform", "embed", "upsert"]
    present = [s for s in expected if s in stages_by_name]
    missing = [s for s in expected if s not in stages_by_name]

    if missing:
        missing_labels = {"load": "ğŸ“„ åŠ è½½", "split": "âœ‚ï¸ åˆ†å—", "transform": "ğŸ”„ è½¬æ¢", "embed": "ğŸ”¢ ç¼–ç ", "upsert": "ğŸ’¾ å­˜å‚¨"}
        names = ", ".join(missing_labels.get(m, m) for m in missing)
        if "load" in missing:
            st.error(
                f"**æµæ°´çº¿ä¸å®Œæ•´ â€” ç¼ºå°‘é˜¶æ®µ: {names}ã€‚** "
                "åŠ è½½é˜¶æ®µå¤±è´¥æˆ–è¢«è·³è¿‡ã€‚æ–‡æ¡£å¯èƒ½å·²æŸåæˆ–æ ¼å¼ä¸å—æ”¯æŒã€‚"
            )
        else:
            st.warning(
                f"**æµæ°´çº¿ä¸å®Œæ•´ â€” ç¼ºå°‘é˜¶æ®µ: {names}ã€‚** "
                "å¤„ç†è¿‡ç¨‹ä¸­å¯èƒ½å‘ç”Ÿäº†é”™è¯¯ã€‚è¯·æŸ¥çœ‹æ—¥å¿—äº†è§£è¯¦æƒ…ã€‚"
            )

    # Stage-specific diagnostics
    if "load" in stages_by_name and load_d.get("text_length", 0) == 0:
        st.warning("**åŠ è½½é˜¶æ®µè¾“å‡ºäº†ç©ºæ–‡æœ¬ã€‚** æ–‡æ¡£å¯èƒ½ä»…åŒ…å«å›¾ç‰‡æˆ–æ ¼å¼ä¸å—æ”¯æŒã€‚")

    if "split" in stages_by_name and split_d.get("chunk_count", 0) == 0:
        st.warning("**åˆ†å—é˜¶æ®µäº§ç”Ÿäº† 0 ä¸ªåˆ†å—ã€‚** æ–‡æ¡£æ–‡æœ¬å¯èƒ½å¤ªçŸ­æˆ–ä¸ºç©ºã€‚")

    if "transform" in stages_by_name:
        refined_llm = transform_d.get("refined_by_llm", 0)
        refined_rule = transform_d.get("refined_by_rule", 0)
        if refined_llm == 0 and refined_rule == 0:
            st.info("**è½¬æ¢:** æ²¡æœ‰åˆ†å—è¢«ä¼˜åŒ–ã€‚LLM ä¼˜åŒ–å¯èƒ½æœªå¯ç”¨æˆ–çŸ­åˆ†å—è¢«è·³è¿‡ã€‚")

    if "embed" in stages_by_name and embed_d.get("dense_vector_count", 0) == 0:
        st.warning("**ç¼–ç é˜¶æ®µäº§ç”Ÿäº† 0 ä¸ªå‘é‡ã€‚** Embedding API å¯èƒ½è°ƒç”¨å¤±è´¥ã€‚è¯·æ£€æŸ¥ API Key å’Œç«¯ç‚¹ã€‚")

    if "upsert" in stages_by_name:
        vec_count = upsert_d.get("vector_count", upsert_d.get("dense_store", {}).get("count", 0))
        if vec_count == 0:
            st.warning("**å­˜å‚¨é˜¶æ®µå†™å…¥äº† 0 ä¸ªå‘é‡ã€‚** æ•°æ®åº“å†™å…¥å¯èƒ½å¤±è´¥ã€‚")

    # Check for error fields in any stage data
    for stage_name in present:
        stage_data = stages_by_name[stage_name].get("data", {})
        err = stage_data.get("error", "")
        if err:
            label = stage_name.replace("_", " ").title()
            st.error(f"**{label} é˜¶æ®µé”™è¯¯:** {err}")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Per-stage renderers
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def _render_load_stage(data: Dict[str, Any], *, trace_idx: int = 0) -> None:
    """Render Load stage: raw document preview."""
    c1, c2, c3 = st.columns(3)
    with c1:
        st.metric("æ–‡æ¡£ ID", data.get("doc_id", "â€”")[:16])
    with c2:
        st.metric("æ–‡æœ¬é•¿åº¦", f"{data.get('text_length', 0):,}")
    with c3:
        st.metric("å›¾ç‰‡æ•°", data.get("image_count", 0))

    preview = data.get("text_preview", "")
    if preview:
        st.markdown("**åŸå§‹æ–‡æ¡£æ–‡æœ¬**")
        st.text_area(
            "raw_text",
            value=preview,
            height=max(120, min(len(preview) // 2, 600)),
            disabled=True,
            label_visibility="collapsed",
            key=f"load_raw_text_{trace_idx}",
        )
    else:
        st.info("æ­¤è¿½è¸ªè®°å½•ä¸­æ²¡æœ‰æ–‡æœ¬é¢„è§ˆã€‚")


def _render_split_stage(data: Dict[str, Any], *, trace_idx: int = 0) -> None:
    """Render Split stage: chunk list with texts."""
    c1, c2 = st.columns(2)
    with c1:
        st.metric("åˆ†å—æ•°", data.get("chunk_count", 0))
    with c2:
        st.metric("å¹³å‡å¤§å°", f"{data.get('avg_chunk_size', 0)} å­—ç¬¦")

    chunks = data.get("chunks", [])
    if chunks:
        st.markdown("**åˆ†å—åçš„ç»“æœ**")
        for i, chunk in enumerate(chunks):
            char_len = chunk.get("char_len", 0)
            chunk_id = chunk.get("chunk_id", "")
            text = chunk.get("text", "")
            header = f"ğŸ“ **åˆ†å— #{i+1}** â€” `{chunk_id[:20]}` â€” {char_len} å­—ç¬¦"
            with st.expander(header, expanded=(i < 2)):
                st.text_area(
                    f"split_{i}",
                    value=text,
                    height=max(100, min(len(text) // 2, 500)),
                    disabled=True,
                    label_visibility="collapsed",
                    key=f"split_{trace_idx}_{i}",
                )
    else:
        st.info("æ²¡æœ‰è®°å½•åˆ†å—æ–‡æœ¬ã€‚è¯·é‡æ–°è¿è¡Œæ‘„å–ä»¥ç”Ÿæˆæ–°çš„è¿½è¸ªã€‚")


def _render_transform_stage(data: Dict[str, Any], *, trace_idx: int = 0) -> None:
    """Render Transform stage: before/after refinement + enrichment metadata."""
    # Summary metrics
    c1, c2, c3 = st.columns(3)
    with c1:
        st.metric(
            "ä¼˜åŒ– (LLM / è§„åˆ™)",
            f"{data.get('refined_by_llm', 0)} / {data.get('refined_by_rule', 0)}",
        )
    with c2:
        st.metric(
            "å¢å¼º (LLM / è§„åˆ™)",
            f"{data.get('enriched_by_llm', 0)} / {data.get('enriched_by_rule', 0)}",
        )
    with c3:
        st.metric("å·²æ ‡æ³¨", data.get("captioned_chunks", 0))

    chunks = data.get("chunks", [])
    if chunks:
        st.markdown("**å„åˆ†å—è½¬æ¢ç»“æœ**")
        for i, chunk in enumerate(chunks):
            chunk_id = chunk.get("chunk_id", "")
            refined_by = chunk.get("refined_by", "")
            enriched_by = chunk.get("enriched_by", "")
            title = chunk.get("title", "")
            tags = chunk.get("tags", [])
            summary = chunk.get("summary", "")
            text_before = chunk.get("text_before", "")
            text_after = chunk.get("text_after", "")

            badge_parts = []
            if refined_by:
                badge_parts.append(f"ä¼˜åŒ–:`{refined_by}`")
            if enriched_by:
                badge_parts.append(f"å¢å¼º:`{enriched_by}`")
            badges = " Â· ".join(badge_parts)

            header = f"ğŸ”„ **åˆ†å— #{i+1}** â€” `{chunk_id[:20]}` â€” {badges}"
            with st.expander(header, expanded=(i == 0)):
                # Metadata from enrichment
                if title or tags or summary:
                    st.markdown("**å¢å¼ºå…ƒæ•°æ®**")
                    meta_cols = st.columns(3)
                    with meta_cols[0]:
                        st.markdown(f"**æ ‡é¢˜:** {title}" if title else "_æ— æ ‡é¢˜_")
                    with meta_cols[1]:
                        if tags:
                            st.markdown("**æ ‡ç­¾:** " + ", ".join(f"`{t}`" for t in tags))
                        else:
                            st.markdown("_æ— æ ‡ç­¾_")
                    with meta_cols[2]:
                        if summary:
                            st.markdown(f"**æ‘˜è¦:** {summary}")

                # Before / After text comparison
                if text_before or text_after:
                    st.markdown("**æ–‡æœ¬å¯¹æ¯”**")
                    # Compute a uniform height so both sides match
                    _max_len = max(len(text_before or ""), len(text_after or ""))
                    _h = max(150, min(_max_len // 2, 600))
                    col_before, col_after = st.columns(2)
                    with col_before:
                        st.markdown("*ä¼˜åŒ–å‰:*")
                        st.text_area(
                            f"before_{i}",
                            value=text_before if text_before else "(ç©º)",
                            height=_h,
                            disabled=True,
                            label_visibility="collapsed",
                            key=f"transform_before_{trace_idx}_{i}",
                        )
                    with col_after:
                        st.markdown("*ä¼˜åŒ– + å¢å¼ºå:*")
                        st.text_area(
                            f"after_{i}",
                            value=text_after if text_after else "(ç©º)",
                            height=_h,
                            disabled=True,
                            label_visibility="collapsed",
                            key=f"transform_after_{trace_idx}_{i}",
                        )
    else:
        st.info("æ²¡æœ‰è®°å½•åˆ†å—è½¬æ¢æ•°æ®ã€‚è¯·é‡æ–°è¿è¡Œæ‘„å–ä»¥ç”Ÿæˆæ–°çš„è¿½è¸ªã€‚")


def _render_embed_stage(data: Dict[str, Any]) -> None:
    """Render Embed stage: dual-path Dense + Sparse encoding details."""
    # â”€â”€ Overview metrics â”€â”€
    c1, c2, c3, c4 = st.columns(4)
    with c1:
        st.metric("ç¨ å¯†å‘é‡", data.get("dense_vector_count", 0))
    with c2:
        st.metric("ç»´åº¦", data.get("dense_dimension", 0))
    with c3:
        st.metric("ç¨€ç–æ–‡æ¡£", data.get("sparse_doc_count", 0))
    with c4:
        st.metric("æ–¹æ³•", data.get("method", "â€”"))

    chunks = data.get("chunks", [])
    if not chunks:
        st.info("æ²¡æœ‰è®°å½•åˆ†å—ç¼–ç æ•°æ®ã€‚")
        return

    # â”€â”€ Dual-path per-chunk table â”€â”€
    st.markdown("---")
    dense_tab, sparse_tab = st.tabs(["ğŸŸ¦ ç¨ å¯†ç¼–ç ", "ğŸŸ¨ ç¨€ç–ç¼–ç  (BM25)"])

    with dense_tab:
        st.markdown("æ¯ä¸ªåˆ†å— â†’ é€šè¿‡ Embedding æ¨¡å‹ç”Ÿæˆ**æµ®ç‚¹å‘é‡**")
        dense_rows = []
        for i, chunk in enumerate(chunks):
            char_len = chunk.get("char_len", 0)
            dense_rows.append({
                "#": i + 1,
                "åˆ†å— ID": chunk.get("chunk_id", ""),
                "å­—ç¬¦æ•°": char_len,
                "é¢„ä¼° Token": max(1, char_len // 3),
                "ç¨ å¯†ç»´åº¦": chunk.get("dense_dim", data.get("dense_dimension", "â€”")),
            })
        st.table(dense_rows)

    with sparse_tab:
        st.markdown("æ¯ä¸ªåˆ†å— â†’ ç”¨äº BM25 ç´¢å¼•çš„**è¯é¢‘ç»Ÿè®¡**")
        sparse_rows = []
        for i, chunk in enumerate(chunks):
            sparse_rows.append({
                "#": i + 1,
                "åˆ†å— ID": chunk.get("chunk_id", ""),
                "æ–‡æ¡£é•¿åº¦ (è¯æ•°)": chunk.get("doc_length", "â€”"),
                "å”¯ä¸€è¯æ•°": chunk.get("unique_terms", "â€”"),
            })
        st.table(sparse_rows)

        # Top terms per chunk
        for i, chunk in enumerate(chunks):
            top_terms = chunk.get("top_terms", [])
            if top_terms:
                with st.expander(f"ğŸ”¤ åˆ†å— {i + 1} â€” é«˜é¢‘è¯", expanded=False):
                    term_rows = [{"è¯": t["term"], "é¢‘æ¬¡": t["freq"]} for t in top_terms]
                    st.table(term_rows)


def _render_upsert_stage(data: Dict[str, Any]) -> None:
    """Render Upsert stage: per-store details with chunk mapping."""
    dense_store = data.get("dense_store", {})
    sparse_store = data.get("sparse_store", {})
    image_store = data.get("image_store", {})

    # â”€â”€ Overview metrics â”€â”€
    c1, c2, c3 = st.columns(3)
    with c1:
        st.metric("ç¨ å¯†å‘é‡", dense_store.get("count", data.get("vector_count", 0)))
    with c2:
        st.metric("ç¨€ç– (BM25)", sparse_store.get("count", data.get("bm25_docs", 0)))
    with c3:
        st.metric("å›¾ç‰‡", image_store.get("count", data.get("images_indexed", 0)))

    # â”€â”€ Dense store details â”€â”€
    if dense_store:
        with st.expander("ğŸŸ¦ ç¨ å¯†å‘é‡å­˜å‚¨ (ChromaDB)", expanded=True):
            dc1, dc2 = st.columns(2)
            with dc1:
                st.markdown(f"**åç«¯:** `{dense_store.get('backend', 'â€”')}`")
                st.markdown(f"**é›†åˆ:** `{dense_store.get('collection', 'â€”')}`")
            with dc2:
                st.markdown(f"**è·¯å¾„:** `{dense_store.get('path', 'â€”')}`")
                st.markdown(f"**å‘é‡æ•°:** {dense_store.get('count', 0)}")

    # â”€â”€ Sparse store details â”€â”€
    if sparse_store:
        with st.expander("ğŸŸ¨ ç¨€ç–ç´¢å¼• (BM25)", expanded=True):
            sc1, sc2 = st.columns(2)
            with sc1:
                st.markdown(f"**åç«¯:** `{sparse_store.get('backend', 'â€”')}`")
                st.markdown(f"**é›†åˆ:** `{sparse_store.get('collection', 'â€”')}`")
            with sc2:
                st.markdown(f"**è·¯å¾„:** `{sparse_store.get('path', 'â€”')}`")
                st.markdown(f"**æ–‡æ¡£æ•°:** {sparse_store.get('count', 0)}")

    # â”€â”€ Image store details â”€â”€
    if image_store and image_store.get("count", 0) > 0:
        with st.expander(f"ğŸ–¼ï¸ å›¾ç‰‡å­˜å‚¨ ({image_store.get('count', 0)} å¼ å›¾ç‰‡)", expanded=True):
            st.markdown(f"**åç«¯:** `{image_store.get('backend', 'â€”')}`")
            imgs = image_store.get("images", [])
            if imgs:
                img_rows = [
                    {
                        "å›¾ç‰‡ ID": img.get("image_id", ""),
                        "é¡µç ": img.get("page", 0),
                        "æ–‡ä»¶": img.get("file_path", ""),
                        "æ–‡æ¡£å“ˆå¸Œ": img.get("doc_hash", "")[:16] + "â€¦",
                    }
                    for img in imgs
                ]
                st.table(img_rows)

    # â”€â”€ Chunk â†’ Vector ID mapping â”€â”€
    chunk_mapping = data.get("chunk_mapping", [])
    if chunk_mapping:
        with st.expander(f"ğŸ”— åˆ†å— â†’ å‘é‡æ˜ å°„ ({len(chunk_mapping)} æ¡)", expanded=False):
            mapping_rows = [
                {
                    "#": i + 1,
                    "åˆ†å— ID": m.get("chunk_id", ""),
                    "å‘é‡ ID": m.get("vector_id", ""),
                    "å­˜å‚¨": m.get("store", ""),
                    "é›†åˆ": m.get("collection", ""),
                }
                for i, m in enumerate(chunk_mapping)
            ]
            st.table(mapping_rows)

    # â”€â”€ Fallback: legacy format with just vector_ids â”€â”€
    if not chunk_mapping and not dense_store:
        vector_ids = data.get("vector_ids", [])
        if vector_ids:
            with st.expander("å‘é‡ ID", expanded=False):
                for vid in vector_ids:
                    st.code(vid, language=None)
