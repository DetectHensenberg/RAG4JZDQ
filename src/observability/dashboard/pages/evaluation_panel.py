"""Evaluation Panel page â€“ run RAG evaluations against a golden test set.

Layout:
1. Evaluator configuration (backend, top-k, collection)
2. Run button with progress spinner
3. Results section: aggregate metrics, per-query detail table
4. Optional: historical evaluation results comparison
"""

from __future__ import annotations

import json
import logging
import time
from pathlib import Path
from typing import Any, Dict, List, Optional

import streamlit as st

logger = logging.getLogger(__name__)

# Default golden test set location
DEFAULT_GOLDEN_SET = Path("tests/fixtures/golden_test_set.json")
# Evaluation results history file
EVAL_HISTORY_PATH = Path("logs/eval_history.jsonl")


def render() -> None:
    """Render the Evaluation Panel page."""
    st.header("ðŸ“ è¯„ä¼°é¢æ¿")
    st.markdown(
        "é’ˆå¯¹**é»„é‡‘æµ‹è¯•é›†**è¿è¡Œè¯„ä¼°ï¼Œè¡¡é‡æ£€ç´¢å’Œç”Ÿæˆè´¨é‡ã€‚"
        "ç»“æžœåŒ…å«é€æŸ¥è¯¢è¯¦æƒ…å’Œæ±‡æ€»æŒ‡æ ‡ã€‚"
    )

    # â”€â”€ Configuration Section â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    st.subheader("âš™ï¸ é…ç½®")

    col1, col2, col3 = st.columns(3)

    with col1:
        backend = st.selectbox(
            "è¯„ä¼°åŽç«¯",
            options=["custom", "ragas", "composite"],
            index=0,
            key="eval_backend",
            help="é€‰æ‹©è¦ä½¿ç”¨çš„è¯„ä¼°åŽç«¯ã€‚",
        )

    # Show info/warning based on selected backend
    if backend in ("custom", "composite"):
        st.info(
            "â„¹ï¸ **Custom Evaluator** å°šæœªå®Œæˆæ•°æ®é›†å‡†å¤‡ï¼Œå½“å‰ä»…ä¸ºé¢„ç•™æŽ¥å£ã€‚"
            "Custom Evaluator éœ€è¦åœ¨ Golden Test Set ä¸­å¡«å†™ `expected_chunk_ids` "
            "ä½œä¸º ground truth æ‰èƒ½è®¡ç®— hit_rate / MRR æŒ‡æ ‡ã€‚"
            "ç›®å‰å»ºè®®ä½¿ç”¨ **ragas** åŽç«¯è¿›è¡Œè¯„ä¼°ã€‚",
            icon="ðŸš§",
        )

    with col2:
        top_k = st.number_input(
            "Top-K",
            min_value=1,
            max_value=50,
            value=10,
            key="eval_top_k",
            help="æ¯æ¬¡æŸ¥è¯¢æ£€ç´¢çš„åˆ†å—æ•°é‡ã€‚",
        )

    with col3:
        collection = st.text_input(
            "é›†åˆ (å¯é€‰)",
            value="",
            key="eval_collection",
            help="é™åˆ¶æ£€ç´¢åˆ°ç‰¹å®šé›†åˆã€‚",
        )

    # Golden test set file selection
    golden_path_str = st.text_input(
        "é»„é‡‘æµ‹è¯•é›†è·¯å¾„",
        value=str(DEFAULT_GOLDEN_SET),
        key="eval_golden_path",
        help="golden_test_set.json æ–‡ä»¶çš„è·¯å¾„ã€‚",
    )
    golden_path = Path(golden_path_str)

    # Validate golden set exists
    if not golden_path.exists():
        st.warning(
            f"âš ï¸ **æœªæ‰¾åˆ°é»„é‡‘æµ‹è¯•é›†:** `{golden_path}`ã€‚"
            "è¯·åˆ›å»ºåŒ…å«æµ‹è¯•æŸ¥è¯¢å’Œé¢„æœŸç»“æžœçš„ JSON æ–‡ä»¶ã€‚"
            "æ ¼å¼å‚è€ƒ `tests/fixtures/golden_test_set.json`ã€‚"
        )

    # â”€â”€ Run Evaluation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    st.divider()

    run_clicked = st.button(
        "â–¶ï¸  è¿è¡Œè¯„ä¼°",
        type="primary",
        key="eval_run_btn",
        disabled=not golden_path.exists(),
    )

    if run_clicked:
        _run_evaluation(
            backend=backend,
            golden_path=golden_path,
            top_k=int(top_k),
            collection=collection.strip() or None,
        )

    # â”€â”€ Historical Results â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    st.divider()
    _render_history()


def _run_evaluation(
    backend: str,
    golden_path: Path,
    top_k: int,
    collection: Optional[str],
) -> None:
    """Execute an evaluation run and display results."""
    with st.spinner("æ­£åœ¨åŠ è½½è¯„ä¼°å™¨å¹¶è¿è¡Œè¯„ä¼°â€¦"):
        try:
            report_dict = _execute_evaluation(
                backend=backend,
                golden_path=golden_path,
                top_k=top_k,
                collection=collection,
            )
        except Exception as exc:
            st.error(f"âŒ è¯„ä¼°å¤±è´¥: {exc}")
            logger.exception("Evaluation failed")
            return

    # â”€â”€ Display results â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    st.success("âœ… è¯„ä¼°å®Œæˆï¼")

    _render_aggregate_metrics(report_dict)
    _render_query_details(report_dict)

    # Save to history
    _save_to_history(report_dict)


def _execute_evaluation(
    backend: str,
    golden_path: Path,
    top_k: int,
    collection: Optional[str],
) -> Dict[str, Any]:
    """Run the evaluation pipeline and return the report dict."""
    from dataclasses import replace as dc_replace

    from src.core.settings import load_settings
    from src.libs.evaluator.evaluator_factory import EvaluatorFactory
    from src.observability.evaluation.eval_runner import EvalRunner, load_test_set

    settings = load_settings()

    # Override evaluator provider from UI selection
    eval_settings = settings.evaluation
    overridden_eval = type(eval_settings)(
        enabled=True,
        provider=backend,
        metrics=eval_settings.metrics if hasattr(eval_settings, "metrics") else [],
    )
    settings_with_override = dc_replace(settings, evaluation=overridden_eval)

    evaluator = EvaluatorFactory.create(settings_with_override)

    # Try to create HybridSearch (optional)
    target_collection = collection or "default"
    hybrid_search = _try_create_hybrid_search(settings, target_collection)

    runner = EvalRunner(
        settings=settings,
        hybrid_search=hybrid_search,
        evaluator=evaluator,
    )

    report = runner.run(
        test_set_path=golden_path,
        top_k=top_k,
        collection=collection,
    )

    return report.to_dict()


def _try_create_hybrid_search(settings: Any, collection: str = "default") -> Any:
    """Attempt to create a HybridSearch instance.

    Returns None if required dependencies are not available.
    """
    try:
        from src.core.query_engine.query_processor import QueryProcessor
        from src.core.query_engine.hybrid_search import create_hybrid_search
        from src.core.query_engine.dense_retriever import create_dense_retriever
        from src.core.query_engine.sparse_retriever import create_sparse_retriever
        from src.ingestion.storage.bm25_indexer import BM25Indexer
        from src.libs.embedding.embedding_factory import EmbeddingFactory
        from src.libs.vector_store.vector_store_factory import VectorStoreFactory

        vector_store = VectorStoreFactory.create(
            settings, collection_name=collection,
        )
        embedding_client = EmbeddingFactory.create(settings)
        dense_retriever = create_dense_retriever(
            settings=settings,
            embedding_client=embedding_client,
            vector_store=vector_store,
        )
        bm25_indexer = BM25Indexer(index_dir=f"data/db/bm25/{collection}")
        sparse_retriever = create_sparse_retriever(
            settings=settings,
            bm25_indexer=bm25_indexer,
            vector_store=vector_store,
        )
        sparse_retriever.default_collection = collection

        query_processor = QueryProcessor()
        return create_hybrid_search(
            settings=settings,
            query_processor=query_processor,
            dense_retriever=dense_retriever,
            sparse_retriever=sparse_retriever,
        )
    except Exception as exc:
        logger.warning("Could not create HybridSearch: %s", exc)
        return None


def _render_aggregate_metrics(report: Dict[str, Any]) -> None:
    """Display aggregate metrics as metric cards."""
    st.subheader("ðŸ“Š æ±‡æ€»æŒ‡æ ‡")

    agg = report.get("aggregate_metrics", {})

    if not agg:
        st.info("æ— å¯ç”¨çš„æ±‡æ€»æŒ‡æ ‡ã€‚")
        return

    cols = st.columns(min(len(agg), 4))
    for idx, (name, value) in enumerate(sorted(agg.items())):
        with cols[idx % len(cols)]:
            st.metric(
                label=name.replace("_", " ").title(),
                value=f"{value:.4f}",
            )

    st.caption(
        f"è¯„ä¼°å™¨: **{report.get('evaluator_name', 'â€”')}** Â· "
        f"æŸ¥è¯¢æ•°: **{report.get('query_count', 0)}** Â· "
        f"æ€»è€—æ—¶: **{report.get('total_elapsed_ms', 0):.0f} ms**"
    )


def _render_query_details(report: Dict[str, Any]) -> None:
    """Display per-query evaluation results in an expandable table."""
    st.subheader("ðŸ” é€æŸ¥è¯¢è¯¦æƒ…")

    query_results = report.get("query_results", [])
    if not query_results:
        st.info("æ— é€æŸ¥è¯¢ç»“æžœã€‚")
        return

    for idx, qr in enumerate(query_results):
        query = qr.get("query", "â€”")
        elapsed = qr.get("elapsed_ms", 0)
        metrics = qr.get("metrics", {})

        # Build metric summary for the expander label
        metric_summary = " Â· ".join(
            f"{k}: {v:.3f}" for k, v in sorted(metrics.items())
        )
        if not metric_summary:
            metric_summary = "æ— æŒ‡æ ‡"

        with st.expander(
            f"**Q{idx + 1}**: {query[:80]} â€” {elapsed:.0f} ms â€” {metric_summary}",
            expanded=False,
        ):
            # Metrics
            if metrics:
                mcols = st.columns(min(len(metrics), 4))
                for midx, (mname, mval) in enumerate(sorted(metrics.items())):
                    with mcols[midx % len(mcols)]:
                        st.metric(mname, f"{mval:.4f}")

            # Retrieved chunks
            chunks = qr.get("retrieved_chunk_ids", [])
            if chunks:
                st.markdown(f"**æ£€ç´¢åˆ°çš„åˆ†å—** ({len(chunks)}):")
                st.code(", ".join(chunks[:20]), language=None)

            # Generated answer
            answer = qr.get("generated_answer")
            if answer:
                st.markdown("**ç”Ÿæˆçš„å›žç­”:**")
                st.text(answer[:500])


def _render_history() -> None:
    """Display historical evaluation results for comparison."""
    st.subheader("ðŸ“ˆ è¯„ä¼°åŽ†å²")

    history = _load_history()
    if not history:
        st.info(
            "**æš‚æ— è¯„ä¼°åŽ†å²ã€‚** "
            "åœ¨ä¸Šæ–¹é…ç½®è¯„ä¼°å™¨å¹¶ç‚¹å‡»ã€Žè¿è¡Œè¯„ä¼°ã€å¼€å§‹ã€‚"
            "ç»“æžœå°†ä¿å­˜åœ¨æ­¤å¤„ï¼Œæ–¹ä¾¿è·¨æ¬¡è¿è¡Œå¯¹æ¯”ã€‚"
        )
        return

    # Show recent runs as a table
    rows = []
    for entry in history[-10:]:  # last 10 runs
        rows.append(
            {
                "æ—¶é—´": entry.get("timestamp", "â€”"),
                "è¯„ä¼°å™¨": entry.get("evaluator_name", "â€”"),
                "æŸ¥è¯¢æ•°": entry.get("query_count", 0),
                "è€—æ—¶ (ms)": round(entry.get("total_elapsed_ms", 0)),
                **{
                    k: round(v, 4)
                    for k, v in entry.get("aggregate_metrics", {}).items()
                },
            }
        )

    st.dataframe(rows, use_container_width=True)


def _save_to_history(report: Dict[str, Any]) -> None:
    """Append an evaluation report to the history file."""
    try:
        EVAL_HISTORY_PATH.parent.mkdir(parents=True, exist_ok=True)
        entry = {
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            **report,
        }
        with EVAL_HISTORY_PATH.open("a", encoding="utf-8") as f:
            f.write(json.dumps(entry, ensure_ascii=False) + "\n")
    except Exception as exc:
        logger.warning("Failed to save evaluation history: %s", exc)


def _load_history() -> List[Dict[str, Any]]:
    """Load evaluation history from JSONL file."""
    if not EVAL_HISTORY_PATH.exists():
        return []

    entries: List[Dict[str, Any]] = []
    try:
        with EVAL_HISTORY_PATH.open("r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if line:
                    try:
                        entries.append(json.loads(line))
                    except json.JSONDecodeError:
                        continue
    except Exception as exc:
        logger.warning("Failed to load evaluation history: %s", exc)

    return entries
